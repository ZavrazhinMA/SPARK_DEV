{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58613a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-SHQVOKD:4040\n",
       "SparkContext available as 'sc' (version = 3.5.0, master = local[*], app id = local-1696340796070)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "import spark.implicits._\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99652243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+--------+-------+----------+\n",
      "|   firstName|lastName|state|quantity|revenue| timestamp|\n",
      "+------------+--------+-----+--------+-------+----------+\n",
      "|Jean-Georges|  Perrin|   NC|       1|    300|1551903533|\n",
      "|Jean-Georges|  Perrin|   NC|       2|    120|1551903567|\n",
      "|Jean-Georges|  Perrin|   CA|       4|     75|1551903599|\n",
      "|      Holden|   Karau|   CA|       6|     37|1551904299|\n",
      "|       Ginni| Rometty|   NY|       7|     91|1551916792|\n",
      "|      Holden|   Karau|   CA|       4|    153|1552876129|\n",
      "+------------+--------+-----+--------+-------+----------+\n",
      "\n",
      "root\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- quantity: integer (nullable = false)\n",
      " |-- revenue: integer (nullable = false)\n",
      " |-- timestamp: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [firstName: string, lastName: string ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq(\n",
    "    (\"Jean-Georges\", \"Perrin\", \"NC\", 1, 300, 1551903533),\n",
    "    (\"Jean-Georges\", \"Perrin\", \"NC\", 2, 120, 1551903567),\n",
    "    (\"Jean-Georges\", \"Perrin\", \"CA\" ,4, 75, 1551903599),\n",
    "    (\"Holden\", \"Karau\", \"CA\" , 6, 37, 1551904299),\n",
    "    (\"Ginni\", \"Rometty\", \"NY\", 7, 91, 1551916792),\n",
    "    (\"Holden\", \"Karau\", \"CA\", 4, 153, 1552876129)\n",
    ").toDF(\"firstName\", \"lastName\", \"state\", \"quantity\", \"revenue\", \"timestamp\")\n",
    "\n",
    "data.show()\n",
    "data.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb50cf0",
   "metadata": {},
   "source": [
    "## ARRAY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba6248f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+--------+-------+----------+--------------------+--------------------+\n",
      "|   firstName|lastName|state|quantity|revenue| timestamp|                 k2v|                 k2n|\n",
      "+------------+--------+-----+--------+-------+----------+--------------------+--------------------+\n",
      "|Jean-Georges|  Perrin|   NC|       1|    300|1551903533|[Jean-Georges, Pe...|[FirstName, LastN...|\n",
      "|Jean-Georges|  Perrin|   NC|       2|    120|1551903567|[Jean-Georges, Pe...|[FirstName, LastN...|\n",
      "+------------+--------+-----+--------+-------+----------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datak2: org.apache.spark.sql.DataFrame = [firstName: string, lastName: string ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datak2 = data\n",
    "                .withColumn(\"k2v\", array($\"firstName\", $\"lastName\", $\"state\"))\n",
    "                .withColumn(\"k2n\", array(lit(\"FirstName\"), lit(\"LastName\"), lit(\"State\")))\n",
    "\n",
    "datak2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa69a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+--------+-------+----------+--------------------+--------------------+\n",
      "|   firstName|lastName|state|quantity|revenue| timestamp|                 k2v|                 k2n|\n",
      "+------------+--------+-----+--------+-------+----------+--------------------+--------------------+\n",
      "|Jean-Georges|  Perrin|   CA|       4|     75|1551903599|[Jean-Georges, Pe...|[FirstName, LastN...|\n",
      "|      Holden|   Karau|   CA|       6|     37|1551904299| [Holden, Karau, CA]|[FirstName, LastN...|\n",
      "+------------+--------+-----+--------+-------+----------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datak2.where(array_contains($\"k2v\", \"CA\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa01e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+--------+-------+----------+--------------------+\n",
      "|   firstName|lastName|state|quantity|revenue| timestamp|                 k2m|\n",
      "+------------+--------+-----+--------+-------+----------+--------------------+\n",
      "|Jean-Georges|  Perrin|   NC|       1|    300|1551903533|{FirstName -> Jea...|\n",
      "|Jean-Georges|  Perrin|   NC|       2|    120|1551903567|{FirstName -> Jea...|\n",
      "+------------+--------+-----+--------+-------+----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datak2m: org.apache.spark.sql.DataFrame = [firstName: string, lastName: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datak2m = data.withColumn(\"k2m\", map(lit(\"FirstName\"), $\"firstName\", lit(\"LastName\"), $\"lastName\", lit(\"State\"), $\"state\"))\n",
    "datak2m.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac4d22",
   "metadata": {},
   "source": [
    "### Date and TimeStamp functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "583ec79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+--------+-------+----------+-------------------+--------------------+\n",
      "|   firstName|lastName|state|quantity|revenue| timestamp|    recordTimeStamp|             current|\n",
      "+------------+--------+-----+--------+-------+----------+-------------------+--------------------+\n",
      "|Jean-Georges|  Perrin|   NC|       1|    300|1551903533|2019-03-06 23:18:53|2023-10-03 16:46:...|\n",
      "|Jean-Georges|  Perrin|   NC|       2|    120|1551903567|2019-03-06 23:19:27|2023-10-03 16:46:...|\n",
      "+------------+--------+-----+--------+-------+----------+-------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datat: org.apache.spark.sql.DataFrame = [firstName: string, lastName: string ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datat = data\n",
    "    .withColumn(\"recordTimeStamp\", to_timestamp($\"timestamp\"))\n",
    "    .withColumn(\"current\", current_timestamp)\n",
    "datat.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4234be2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------+\n",
      "|    recordTimeStamp|             current|date_diff|\n",
      "+-------------------+--------------------+---------+\n",
      "|2019-03-06 23:18:53|2023-10-03 16:46:...|     1672|\n",
      "|2019-03-06 23:19:27|2023-10-03 16:46:...|     1672|\n",
      "+-------------------+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datat.select($\"recordTimeStamp\", $\"current\", datediff( $\"current\", $\"recordTimeStamp\").as(\"date_diff\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab6d13c",
   "metadata": {},
   "source": [
    "### JSON functons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "712530ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+--------+-------+----------+--------------------+--------------------+\n",
      "|   firstName|lastName|state|quantity|revenue| timestamp|                 k2m|                 k2j|\n",
      "+------------+--------+-----+--------+-------+----------+--------------------+--------------------+\n",
      "|Jean-Georges|  Perrin|   NC|       1|    300|1551903533|{FirstName -> Jea...|{\"FirstName\":\"Jea...|\n",
      "|Jean-Georges|  Perrin|   NC|       2|    120|1551903567|{FirstName -> Jea...|{\"FirstName\":\"Jea...|\n",
      "+------------+--------+-----+--------+-------+----------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datak2j: org.apache.spark.sql.DataFrame = [firstName: string, lastName: string ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datak2j = datak2m.withColumn(\"k2j\", to_json($\"k2m\"))\n",
    "datak2j.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e42d61",
   "metadata": {},
   "source": [
    "### GENERATOR Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2936c9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+--------+-------+----------+---------------+---------------+\n",
      "|   firstName|lastName|state|quantity|revenue| timestamp|             a1|             a2|\n",
      "+------------+--------+-----+--------+-------+----------+---------------+---------------+\n",
      "|Jean-Georges|  Perrin|   NC|       1|    300|1551903533|[1, 2, 3, 4, 5]|[1, 2, 3, 4, 5]|\n",
      "|Jean-Georges|  Perrin|   NC|       2|    120|1551903567|[1, 2, 3, 4, 5]|[1, 2, 3, 4, 5]|\n",
      "+------------+--------+-----+--------+-------+----------+---------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data2: org.apache.spark.sql.DataFrame = [firstName: string, lastName: string ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data2 = data\n",
    "            .withColumn(\"a1\", array(lit(1), lit(2), lit(3), lit(4), lit(5)))\n",
    "            .withColumn(\"a2\", lit((1 to 5).toArray))\n",
    "data2.show(2)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a1813cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+--------+-------+----------+---------------+---------------+-----+\n",
      "|   firstName|lastName|state|quantity|revenue| timestamp|             a1|             a2|duumy|\n",
      "+------------+--------+-----+--------+-------+----------+---------------+---------------+-----+\n",
      "|Jean-Georges|  Perrin|   NC|       1|    300|1551903533|[1, 2, 3, 4, 5]|[1, 2, 3, 4, 5]|    1|\n",
      "|Jean-Georges|  Perrin|   NC|       1|    300|1551903533|[1, 2, 3, 4, 5]|[1, 2, 3, 4, 5]|    2|\n",
      "|Jean-Georges|  Perrin|   NC|       1|    300|1551903533|[1, 2, 3, 4, 5]|[1, 2, 3, 4, 5]|    3|\n",
      "|Jean-Georges|  Perrin|   NC|       1|    300|1551903533|[1, 2, 3, 4, 5]|[1, 2, 3, 4, 5]|    4|\n",
      "|Jean-Georges|  Perrin|   NC|       1|    300|1551903533|[1, 2, 3, 4, 5]|[1, 2, 3, 4, 5]|    5|\n",
      "+------------+--------+-----+--------+-------+----------+---------------+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2.withColumn(\"duumy\", explode($\"a1\")).show(5) // "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b37ed9d",
   "metadata": {},
   "source": [
    "### WINDOW Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d025582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9a77880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+--------+-------+----------+---------+\n",
      "|   firstName|lastName|state|quantity|revenue| timestamp|row_nuber|\n",
      "+------------+--------+-----+--------+-------+----------+---------+\n",
      "|       Ginni| Rometty|   NY|       7|     91|1551916792|        1|\n",
      "|      Holden|   Karau|   CA|       6|     37|1551904299|        1|\n",
      "|      Holden|   Karau|   CA|       4|    153|1552876129|        2|\n",
      "|Jean-Georges|  Perrin|   CA|       4|     75|1551903599|        1|\n",
      "|Jean-Georges|  Perrin|   NC|       1|    300|1551903533|        2|\n",
      "+------------+--------+-----+--------+-------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wind: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@44894fc0\r\n",
       "dataw: org.apache.spark.sql.DataFrame = [firstName: string, lastName: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wind = Window.partitionBy(\"firstName\", \"lastName\").orderBy(\"state\")\n",
    "val dataw = data.withColumn(\"row_nuber\", row_number.over(wind))\n",
    "dataw.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d101fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+--------+-------+----------+---------+\n",
      "|   firstName|lastName|state|quantity|revenue| timestamp|row_nuber|\n",
      "+------------+--------+-----+--------+-------+----------+---------+\n",
      "|       Ginni| Rometty|   NY|       7|     91|1551916792|        1|\n",
      "|      Holden|   Karau|   CA|       6|     37|1551904299|        1|\n",
      "|Jean-Georges|  Perrin|   CA|       4|     75|1551903599|        1|\n",
      "+------------+--------+-----+--------+-------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataw.where($\"row_nuber\" === 1).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3926f9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+--------+-------+----------+---------+--------+\n",
      "|firstName|lastName|state|quantity|revenue| timestamp|row_nuber|revenue2|\n",
      "+---------+--------+-----+--------+-------+----------+---------+--------+\n",
      "|    Ginni| Rometty|   NY|       7|     91|1551916792|        1|    NULL|\n",
      "|   Holden|   Karau|   CA|       6|     37|1551904299|        1|     153|\n",
      "|   Holden|   Karau|   CA|       4|    153|1552876129|        2|     153|\n",
      "+---------+--------+-----+--------+-------+----------+---------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataw.withColumn(\"revenue2\", nth_value($\"revenue\", 2).over(wind)).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2e4fd3",
   "metadata": {},
   "source": [
    "## UDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d4e2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|   name|salary|\n",
      "+---+-------+------+\n",
      "|  1|Michael|  3000|\n",
      "|  2|   Andy|  4500|\n",
      "|  3| Justin|  3500|\n",
      "|  4|  Berta|  4000|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq((1, \"Michael\", 3000), (2, \"Andy\", 4500), (3, \"Justin\", 3500), (4, \"Berta\", 4000)).toDF(\"id\", \"name\", \"salary\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56a0843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-------------------+\n",
      "| id|   name|salary|             random|\n",
      "+---+-------+------+-------------------+\n",
      "|  1|Michael|  3000|0.13560989285697567|\n",
      "|  2|   Andy|  4500| 0.3569374509566343|\n",
      "|  3| Justin|  3500|0.24174624042407888|\n",
      "|  4|  Berta|  4000|  0.745762166700845|\n",
      "+---+-------+------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rand: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4625/0x0000000101841c40@19fd2cf1,DoubleType,List(),Some(class[value[0]: double]),None,false,true)\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// без аргументов\n",
    "val rand = udf(() => Math.random())\n",
    "data.withColumn(\"random\", random()).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d4f1b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|salary|  NS|\n",
      "+------+----+\n",
      "|  3000|3001|\n",
      "|  4500|4501|\n",
      "|  3500|3501|\n",
      "|  4000|4001|\n",
      "+------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "plusOne: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4653/0x000000010181a840@3c777ab6,IntegerType,List(Some(class[value[0]: int])),Some(class[value[0]: int]),None,false,true)\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 1 аргумент\n",
    "val plusOne = udf((x: Int) => x + 1)\n",
    "data.select($\"salary\", plusOne($\"salary\").as(\"NS\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a6d170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-------+------+\n",
      "| id|length(name)|   name|result|\n",
      "+---+------------+-------+------+\n",
      "|  1|           7|Michael|     8|\n",
      "|  2|           4|   Andy|     5|\n",
      "|  3|           6| Justin|     7|\n",
      "|  4|           5|  Berta|     6|\n",
      "+---+------------+-------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "a2args: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4685/0x00000001017f9040@338c2983,IntegerType,List(Some(class[value[0]: string]), Some(class[value[0]: int])),Some(class[value[0]: int]),None,false,true)\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 2 аргумента\n",
    "val a2args = udf((x: String, y: Int) => x.length + y)\n",
    "data.select($\"id\", length($\"name\"), $\"name\", a2args($\"name\", lit(1)).as(\"result\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "434f651b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|salary|\n",
      "+---+------+------+\n",
      "|  3|Justin|  3500|\n",
      "|  4| Berta|  4000|\n",
      "+---+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "onefilter: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4694/0x00000001017f1840@318b7e8f,BooleanType,List(Some(class[value[0]: int])),Some(class[value[0]: boolean]),None,false,true)\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// булеан для where\n",
    "val onefilter = udf((n: Int) => {n > 2})\n",
    "data.where(onefilter($\"id\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8507555",
   "metadata": {},
   "source": [
    "## UDAF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d25b3",
   "metadata": {},
   "source": [
    "### AGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37b07d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+-----+-------+\n",
      "|   firstName|lastName|state|q_sum|avg_sal|\n",
      "+------------+--------+-----+-----+-------+\n",
      "|Jean-Georges|  Perrin|   NC|    3|  210.0|\n",
      "|Jean-Georges|  Perrin|   CA|    4|   75.0|\n",
      "|      Holden|   Karau|   CA|   10|   95.0|\n",
      "+------------+--------+-----+-----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggDF: org.apache.spark.sql.DataFrame = [firstName: string, lastName: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aggDF = dataw\n",
    "        .groupBy($\"firstName\", $\"lastName\", $\"state\")\n",
    "        .agg(\n",
    "            sum(\"quantity\").as(\"q_sum\"),\n",
    "            avg(\"revenue\").as(\"avg_sal\")\n",
    "        )\n",
    "aggDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01292ea6",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html#udaf[IN,BUF,OUT](agg:org.apache.spark.sql.expressions.Aggregator[IN,BUF,OUT],inputEncoder:org.apache.spark.sql.Encoder[IN]):org.apache.spark.sql.expressions.UserDefinedFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fcc6f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Average\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Average(var sum: Long, var count: Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6acfdbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{Encoder, Encoders}\r\n",
       "import org.apache.spark.sql.expressions.Aggregator\r\n",
       "defined class MyAverage\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{Encoder, Encoders}\n",
    "import org.apache.spark.sql.expressions.Aggregator\n",
    "\n",
    "class MyAverage extends Aggregator[Long, Average, Double] {\n",
    "  // Начальное значение. Должно соответствовать свойству: любое b + zero = b\n",
    "  def zero: Average = Average(0L, 0L)\n",
    "  // Объединение двух значений в новое значение.\n",
    "  // Для повышения производительности функция может изменять `buffer` и\n",
    "  // возвращать его вместо создания нового объекта.\n",
    "  def reduce(buffer: Average, data: Long): Average = {\n",
    "    buffer.sum += data\n",
    "    buffer.count += 1\n",
    "    buffer\n",
    "  }\n",
    "  // Объединение двух промежуточных значения\n",
    "  def merge(b1: Average, b2: Average): Average = {\n",
    "    b1.sum += b2.sum\n",
    "    b1.count += b2.count\n",
    "    b1\n",
    "  }\n",
    "  // Преобразование выходных данных\n",
    "  def finish(reduction: Average): Double =\n",
    "    reduction.sum.toDouble / reduction.count\n",
    "  // Кодировщик для типа промежуточного значения\n",
    "  def bufferEncoder: Encoder[Average] = Encoders.product\n",
    "  // Кодировщик для типа выходного значения\n",
    "  def outputEncoder: Encoder[Double] = Encoders.scalaDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3402bd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.UserDefinedFunction\r\n",
       "import org.apache.spark.sql.functions.udaf\r\n",
       "import org.apache.spark.sql.DataFrame\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.UserDefinedFunction\n",
    "import org.apache.spark.sql.functions.udaf\n",
    "import org.apache.spark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35845f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myAvaregeUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedAggregator(MyAverage@29205348,class[value[0]: bigint],None,true,true)\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myAvaregeUDF: UserDefinedFunction = udaf(new MyAverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2b8e48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|average_salary|\n",
      "+--------------+\n",
      "|        3750.0|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.sql.DataFrame = [average_salary: double]\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result: DataFrame = data.agg(myAvaregeUDF($\"salary\").as(\"average_salary\"))\n",
    "result.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d349158",
   "metadata": {},
   "source": [
    "можно зарегистрирвать функцию и спользовать в sql запросах - смотри документацию UDAF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
